from getPrimedRegion import *
from kmerCounter import *
from cellPrimerTemplTab import *
from numReadsPerCell import *
from ptModel import *
from getCommonPtPairs import *
from makeAvgNErrorMatrix import *
from predictCoverage import *
import multiprocessing
import os

BAMFILE = config['bamfile']
COUTC = config['coutc']
SAMPLE = config['sample']
TYPE = config['type']
DIR = config['dir']
REFGEN = config['refgen']
# CELLS = ['cell' + str(n) for n in range(1,385)]
CELLS=[cell.strip() for cell in open(DIR + '/' + SAMPLE + '.selectedCells.txt').readlines()]


rule all:
    input:
        commonPt = expand('{dir}/{sample}.commonPtPairs.csv', dir=DIR, sample=SAMPLE),
        # predCov = expand('{dir}/predictedCov/{sample}.CovPred.{cell}.qual.txt', dir=DIR, sample=SAMPLE, cell=CELLS)

rule get_primed_region:
    input:
        bam=BAMFILE,
        refgen=REFGEN
    output:
        primedfa= '{dir}/{sample}.primedreg.fa'
    params:
        t=TYPE,
        dir=DIR
    threads: 1
    run:
        get_template_fasta(input.bam, input.refgen, params.dir , params.t)

rule kmer_count: ## Job never finishes. Why oh why.
    input:
        coutc=COUTC,
        refgen=REFGEN
    output:
        cellAbundance='{dir}/{sample}.cellAbundance.noN.csv'
    threads: 8
    run:
        countT = pd.read_csv(input.coutc, sep='\t', index_col=0)
        countDic = count_kmers_refgen(input.refgen)

        workers = multiprocessing.Pool(threads)
        finalKmerCounts = collections.Counter()

        cellDic={}
        for cellCounter in workers.imap_unordered(cellKmersAbundance, [ (countDic, countT, cell) for cell in countT]):
            if len(cellCounter.keys())==1:
                for cell,counter in cellCounter.items():
                    cellDic[cell]=counter
            else:
                print('Something wrong...')

        sample = wildcards.sample

        ab = pd.DataFrame.from_dict(cellDic)
        noN = ab.T[[i for i in ab.index if 'N' not in i and 'Y' not in i]].T
        outputTab = noN.to_csv(wildcards.dir + '/' + sample +'.cellAbundance.noN.csv')

rule num_reads:
    input:
        bam=BAMFILE
    output:
        numReads='{dir}/{sample}.numReads.txt'
    params:
        out=DIR
    threads: 1
    run:
        num_reads_per_cell(input.bam, to_dir = params.out)

rule find_processed_cells:
    input:
        numReads='{dir}/{sample}.numReads.txt'
    output:
        highcovCells='{dir}/{sample}.selectedCells.txt'
    run:
        numreads=pd.read_csv(input.numReads, sep='\t')
        with open(output.highcovCells, 'w') as outfile:
            for cell in numreads[numreads.numReads>10000].cell:
                print(cell, file=outfile)

rule pt_counts:
    input:
        bam=BAMFILE,
        primedfa= '{dir}/{sample}.primedreg.fa',
        cellAbundance='{dir}/{sample}.cellAbundance.noN.csv',
        highcovCells='{dir}/{sample}.selectedCells.txt'
    output:
        ptCounts=expand('{{dir}}/ptCounts/{{sample}}.cell{cell}.ptCounts.qualFilt.parallel.csv', cell=CELLS)
    params:
        type=TYPE
    threads: 10
    run:
        templDic = make_templ_primer_dic(input.bam,input.primedfa, type=params.type)
        if params.type=='rna':
            cellDic = split_pt_dic(templDic)
            # highcovCells = [i for i in cellDic.keys() if len(cellDic[i].values()) > 10000]
            save_ptCounts(cellDic, input.primedfa)

        # if type=='bs':
        #     abundanceFile='mm10.cellAbundance.noN.csv'
        #     tabAb = pd.read_csv(abundanceFile, index_col=0, header=None)
        #     df = make_occurrencies_tbl(templDic)
        #     path = '/'.join(fasta.split('/')[:-1])
        #     sample = bamfile.split('/')[-1].split('.')[0]
        #     df = fillNsortPTmatrix(df, tabAb)
        #     df.to_csv(path + sample + '.ptCounts.qualFilt.parallel.csv')

# rule zipPtCounts:
#     input:
#         ptCounts='{dir}/ptCounts/{sample}.{cell}.ptCounts.qualFilt.parallel.csv'
#     output:
#         ptCountsZipped='{dir}/ptCounts/{sample}.{cell}.ptCounts.qualFilt.parallel.csv.gz'
#     shell:
#         "gzip {input.ptCounts}"

rule predict_dg:
    input:
        ptCounts='{dir}/ptCounts/{sample}.{cell}.ptCounts.qualFilt.parallel.csv',
        cellAbundance='{dir}/{sample}.cellAbundance.noN.csv'
    output:
        predictedDg='{dir}/predictedDg/{sample}_{cell}_ptDg_qual.csv'
    params:
        type=TYPE
    threads: 1
    run:
        if params.type=='rna':
            sample = input.cellAbundance.split('/')[-1].split('.cellAbundance')[0]
            cell = input.ptCounts.split('/')[-1].split('.ptCounts')[0].split('cell')[-1]
            tabAb = pd.read_csv(input.cellAbundance, index_col=0, compression=findCompr(input.cellAbundance))
            cellAb = tabAb[cell]
            ptMat = pd.read_csv(input.ptCounts, compression=findCompr(input.ptCounts), index_col=0)
            dgMat = make_DgMat_per_cell((cellAb, ptMat))
            path = '/'.join(input.cellAbundance.split('/')[:-1])
            if path:
                outpath = path + '/predictedDg/'
            else:
                outpath = './predictedDg/'
            dgMat.to_csv(outpath + sample + '_cell'+ cell +'_ptDg_qual.csv')
        #
        # if type=='bs':
        #     sample = ptMatrix.split('/')[-1].split('.ptCounts')[0]
        #     tabAb = pd.read_csv(cellAbundanceTab, index_col=0, compression=findCompr(cellAbundanceTab), header=None)
        #     genomeAb = tabAb[1]
        #     ptMat = pd.read_csv(ptMatrix, compression=findCompr(ptMatrix), index_col=0)
        #     dgMat = make_DgMat_per_cell((genomeAb, ptMat))
        #     path = '/'.join(ptMatrix.split('/')[:-1])
        #     dgMat.to_csv(path + sample +'_ptDg_qual.csv')


rule commonPtPairs:
    input:
        dgFiles=expand('{{dir}}/predictedDg/{{sample}}_{cell}_ptDg_qual.csv', cell=CELLS),
        highcovCells='{dir}/{sample}.selectedCells.txt'
    output:
        commonPt='{dir}/{sample}.commonPtPairs.csv'
    params:
        ncells=10
    threads: 10
    run:
        path = wildcards.dir
        ncells = params.ncells
        with open(input.highcovCells, 'r') as f:
            cells = [c.strip() for c in f.readlines()]
        selectedFiles=[]
        for cell in cells:
            selectedFiles.extend(fnmatch.filter(input.dgFiles, '*cell'+cell+'*'))
        workers = multiprocessing.Pool(threads)
        finalCellDic = {}
        for dic in workers.imap_unordered(makeNonInfDic, [ file for file in selectedFiles]):
            for pt,celdic in dic.items():
                if pt not in finalCellDic.keys():
                    finalCellDic[pt]={}
                finalCellDic[pt] = dict(finalCellDic[pt],**celdic)

        filtDic={}
        for k,v in finalCellDic.items():
            if len(v) >= ncells:
                filtDic[k] = v

        sample = wildcards.sample
        output = path + '/' + sample + '.commonPtPairs.csv'
        pd.DataFrame.from_dict(filtDic).T.to_csv(output)

## Add rule that removes the useless ptTables

rule makeAvg:
    input:
        commonPt='{dir}/{sample}.commonPtPairs.csv',
        numReads='{dir}/{sample}.numReads.txt'
    output:
        avg='{dir}/{sample}.dgMat.csv',
        err='{dir}/{sample}.errMat.csv'
    run:
        cellAbFile = input.numReads.split('.numReads.txt')[0] + '.cellAbundance.noN.csv'
        cellAb = pd.read_csv(cellAbFile, index_col=0, compression = findCompr(cellAbFile))

        # Make predicted Dg tab
        pairsDg = filter_cells(input.commonPt, input.numReads)
        dgMat,errMat = make_predictedDg_matrix(pairsDg, cellAb)

        outfile = input.numReads.split('.numReads.txt')[0]
        dgMat.to_csv(outfile + '.dgMat.csv')
        errMat.to_csv(outfile +'.errMat.csv')

# rule predictCoverage:
#     input:
#         ptCounts='{dir}/ptCounts/{sample}.{cell}.ptCounts.qualFilt.parallel.csv',
#         avg='{dir}/{sample}.dgMat.csv',
#         err='{dir}/{sample}.errMat.csv'
#     output:
#         predCov='{dir}/predictedCov/{sample}.CovPred.{cell}.qual.txt'
#     params:
#         t=TYPE
#     run:
#         if params.t=='rna':
#             cellAbundanceTab = wildcards.sample + '.cellAbundance.noN.csv'
#             cell = wildcards.cell.split('cell')[1]
#             tabAb = pd.read_csv(cellAbundanceTab, index_col=0, compression = findCompr(cellAbundanceTab))
#             cellAb = tabAb[cell]
#             ptMat = pd.read_csv(input.ptCounts, compression = findCompr(input.ptCounts), index_col=0)
#             errDgMat = pd.read_csv(input.err, compression = findCompr(input.err), index_col=0)
#             dgMat = pd.read_csv(input.avg, index_col=0, compression = findCompr(input.avg))
#             # if thresh:
#             #     dgMat = setThresh4Dg(dgMat,ptMat,thresh=thresh)
#             list=[]
#             with open(wildcards.dir+'/predictedCov/' + wildcards.sample + '.CovPred.cell'+str(cell)+'.qual.txt', 'w') as output:
#                 print('template','obs', 'exp', 'err', sep='\t', file=output)
#                 for templ in dgMat.iterrows():
#                     t,DgRow = templ
#                     print(t, ptMat.loc[ptMat.index==t].fillna(0).values.sum(), predictCov(cellAb[t],DgRow), propagateError(cellAb[t], DgRow, errDgMat.loc[t]), sep='\t', file=output)
#
#         # if params.t=='bs':
#         #     cellAbundanceTab = '/hpc/hub_oudenaarden/edann/hexamers/VAN1667prediction/mm10.cellAbundance.noN.csv'
#         #     tabAb = pd.read_csv(cellAbundanceTab, index_col=0, compression=findCompr(cellAbundanceTab), header=None)
#         #     genomeAb = tabAb[1]
#         #     ptMat = pd.read_csv(ptMatrix, compression=findCompr(ptMatrix), index_col=0)
#         #     errDgMat = pd.read_csv(errDg, compression = findCompr(errDg), index_col=0)
#         #     dgMat = pd.read_csv(path+predictedDg, index_col=0, compression = findCompr(predictedDg))
#         #     path = '/'.join(ptMatrix.split('/')[:-1])
#         #     with open(path+'predictedCov/'+sample+'.CovPred.qual.txt', 'w') as output:
#         #         print('template','obs', 'exp', 'err', sep='\t') #, file=output)
#         #         for templ in dgMat.iterrows():
#         #             t,DgRow = templ
#         #             print(t, ptMat.loc[ptMat.index==t].fillna(0).values.sum(), predictCov(cellAb[t],DgRow)) #, propagateError(cellAb[t], errDgMat[t]), sep='\t')#, file=output)
