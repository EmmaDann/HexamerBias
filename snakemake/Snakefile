from getPrimedRegion import *
from kmerCounter import *
from cellPrimerTemplTab import *
from numReadsPerCell import *
from ptModel import *
from getCommonPtPairs import *
import multiprocessing
import os

SAMPLE = "SvdB11d2-MitoTrackerThird-Satellites-Adult"
TYPE = "rna"
DIR = "/hpc/hub_oudenaarden/edann/hexamers/rnaseq/mouse/SvB11d2"
REFGEN = "/hpc/hub_oudenaarden/edann/hexamers/rnaseq/mouse/mm10_RefSeq_genes_clean_ERCC92_polyA_10_masked_eGFP_Mito.fa"
CELLS = ['cell' + str(n) for n in range(1,385)]

rule all:
    input:
        # primedfa = expand('{dir}/{sample}.primedreg.fa', dir=DIR, sample=SAMPLE),
        # cellAbundance = expand('{dir}/{sample}.cellAbundance.noN.csv', dir=DIR, sample=SAMPLE)
        # ptCounts='{{dir}}/ptCounts/{{sample}}.cell.ptCounts.qualFilt.parallel.csv') #, cell=CELLS)
        # predictedDg=expand('{dir}/predictedDg/{sample}_{cell}_ptDg_qual.csv', cell=CELLS, dir=DIR, sample=SAMPLE),
        commonPt=expand('{dir}/{sample}.commonPtPairs.csv', dir=DIR, sample=SAMPLE)
        # numReads=expand('{dir}/{sample}.numReads.txt', sample=SAMPLE, dir=DIR)
        # # split_bam=expand("bam/{sample}_{read}_bismark_bt2.bam", sample=SAMPLE, read=READS)
        # bam1=expand("bam/{sample}_1_bismark_bt2.bam", sample=SAMPLE)


rule get_primed_region:
    input:
        bam='/hpc/hub_oudenaarden/aalemany/emma-adi/mouse/{sample}.sam.gz',
        refgen=REFGEN
    output:
        primedfa= '{dir}/{sample}.primedreg.fa'
    params:
        t=TYPE,
        dir=DIR
    threads: 1
    run:
        get_template_fasta(input.bam, input.refgen, params.dir , params.t)

rule kmer_count:
    input:
        coutc='/hpc/hub_oudenaarden/aalemany/emma-adi/mouse/{sample}.coutc.csv',
        refgen=REFGEN
    output:
        cellAbundance='{dir}/{sample}.cellAbundance.noN.csv'
    # params:
    #     out=DIR
    threads: 8
    run:
        countT = pd.read_csv(input.coutc, sep='\t', index_col=0)
        countDic = count_kmers_refgen(input.refgen)

        workers = multiprocessing.Pool(threads)
        finalKmerCounts = collections.Counter()

        cellDic={}
        for cellCounter in workers.imap_unordered(cellKmersAbundance, [ (countDic, countT, cell) for cell in countT]):
            if len(cellCounter.keys())==1:
                for cell,counter in cellCounter.items():
                    cellDic[cell]=counter
            else:
                print('Something wrong...')

        sample = input.coutc.split('/')[-1].split('.coutc')[0]
        if args.o:
            outpath = args.o
        else:
            outpath = '/'.join(input.refgen.split('/')[:-1]) + '/'

        ab = pd.DataFrame.from_dict(cellDic)
        noN = ab.T[[i for i in ab.index if 'N' not in i and 'Y' not in i]].T
        print(noN)
        outputTab = noN.to_csv(outpath + '/' + sample +'.cellAbundance.noN.csv')

rule num_reads:
    input:
        bam='/hpc/hub_oudenaarden/aalemany/emma-adi/mouse/{sample}.sam.gz'
    output:
        numReads='{dir}/{sample}.numReads.txt'
    params:
        out=DIR
    threads: 1
    run:
        num_reads_per_cell(input.bam, to_dir = params.dir)

rule find_processed_cells:
    input:
        numReads='{dir}/{sample}.numReads.txt'
    output:
        highcovCells='{dir}/{sample}.selectedCells.txt'
    run:
        numreads=pd.read_csv(input.numReads, sep='\t')
        with open(output.highcovCells, 'w') as outfile:
            for cell in numreads[numreads.numReads>10000].cell:
                print(cell, file=outfile)


rule pt_counts:
    input:
        bam='/hpc/hub_oudenaarden/aalemany/emma-adi/mouse/{sample}.sam.gz',
        primedfa= '{dir}/{sample}.primedreg.fa'
    output:
        ptCounts=expand('{{dir}}/ptCounts/{{sample}}.{cell}.ptCounts.qualFilt.parallel.csv', cell=CELLS)
    params:
        type=TYPE
    threads: 10
    run:
        templDic = make_templ_primer_dic(input.bam,input.primedfa, type=params.type)
        if params.type=='rna':
            cellDic = split_pt_dic(templDic)
            # highcovCells = [i for i in cellDic.keys() if len(cellDic[i].values()) > 10000]
            save_ptCounts(cellDic, input.primedfa)

        # if type=='bs':
        #     abundanceFile='mm10.cellAbundance.noN.csv'
        #     tabAb = pd.read_csv(abundanceFile, index_col=0, header=None)
        #     df = make_occurrencies_tbl(templDic)
        #     path = '/'.join(fasta.split('/')[:-1])
        #     sample = bamfile.split('/')[-1].split('.')[0]
        #     df = fillNsortPTmatrix(df, tabAb)
        #     df.to_csv(path + sample + '.ptCounts.qualFilt.parallel.csv')


rule predict_dg:
    input:
        ptCounts='{dir}/ptCounts/{sample}.{cell}.ptCounts.qualFilt.parallel.csv',
        cellAbundance='{dir}/{sample}.cellAbundance.noN.csv'
    output:
        predictedDg='{dir}/predictedDg/{sample}_{cell}_ptDg_qual.csv'
    params:
        type=TYPE
    threads: 1
    run:
        if params.type=='rna':
            sample = input.cellAbundance.split('/')[-1].split('.cellAbundance')[0]
            cell = input.ptCounts.split('/')[-1].split('.ptCounts')[0].split('cell')[-1]
            tabAb = pd.read_csv(input.cellAbundance, index_col=0, compression=findCompr(input.cellAbundance))
            cellAb = tabAb[cell]
            ptMat = pd.read_csv(input.ptCounts, compression=findCompr(ptMatrix), index_col=0)
            dgMat = make_DgMat_per_cell((cellAb, ptMat))
            path = '/'.join(input.cellAbundance.split('/')[:-1])
            if path:
                outpath = path + '/predictedDg/'
            else:
                outpath = './predictedDg/'
            dgMat.to_csv(outpath + sample + '_cell'+ cell +'_ptDg_qual.csv')
        #
        # if type=='bs':
        #     sample = ptMatrix.split('/')[-1].split('.ptCounts')[0]
        #     tabAb = pd.read_csv(cellAbundanceTab, index_col=0, compression=findCompr(cellAbundanceTab), header=None)
        #     genomeAb = tabAb[1]
        #     ptMat = pd.read_csv(ptMatrix, compression=findCompr(ptMatrix), index_col=0)
        #     dgMat = make_DgMat_per_cell((genomeAb, ptMat))
        #     path = '/'.join(ptMatrix.split('/')[:-1])
        #     dgMat.to_csv(path + sample +'_ptDg_qual.csv')

rule commonPtPairs:
    input:
        dgFiles=expand('{{dir}}/predictedDg/{{sample}}_{cell}_ptDg_qual.csv', cell=CELLS),
        highcovCells='{dir}/{sample}.selectedCells.txt'
    output:
        commonPt='{dir}/{sample}.commonPtPairs.csv'
    params:
        ncells=10
    threads: 10
    run:
        path = wildcards.dir
        ncells = params.ncells
        with open(input.highcovCells, 'r') as f:
            cells=f.readlines()
        selectedFiles=[]
        for cell in cells:
            selectedFiles.extend(fnmatch.filter(dgFiles, '*cell'+cell+'*'))
        workers = multiprocessing.Pool(threads)
        finalCellDic = {}
        for dic in workers.imap_unordered(makeNonInfDic, [ file for file in selectedFiles]):
            for pt,celdic in dic.items():
                if pt not in finalCellDic.keys():
                    finalCellDic[pt]={}
                finalCellDic[pt] = dict(finalCellDic[pt],**celdic)

        filtDic={}
        for k,v in finalCellDic.items():
            if len(v) >= ncells:
                filtDic[k] = v

        sample = wildcards.sample
        output = path + '/' + sample + '.commonPtPairs.csv'
        pd.DataFrame.from_dict(filtDic).T.to_csv(output)

## Add rule that removes the useless ptTables
